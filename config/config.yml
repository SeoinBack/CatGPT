model_params:
    name: 'oc20-2M-GPT2-test'
    architecture : 'GPT'
    use_pretrained : False
    use_lora : False
    use_numerical_encoding : False
    
    n_positions: 1024
    n_embd: 512
    n_layer: 12
    n_head: 8
    d_inner: 1024
    
    # for PML
    noise_density: 0.15
    mean_span: 3
    
    # pretrained 
    checkpoint_path: './model/GPT2-2M/'
    
    # lora
    r: 16,
    lora_alpha: 32,
    lora_dropout: 0.05,
    task_type: 'CAUSAL_LM'
    
data_params:
    string_type: 'coordinate'
    augment_type: 'normal'
    add_props: False
    
    train_data_path: './data/dataset/oc20-2M/train.csv'
    val_data_path: './data/dataset/oc20-2M/val.csv'
    
    # hyperparameters
    num_epochs: 15
    max_len: 1024
    batch_size: 96
    gradient_accumulation_steps: 1
    learning_rate: 1e-3
    warmup_steps: 1000
    
    # etc
    eval_steps: 5000,
    tf32: True
    num_workers: 4
    use_hf_datasets: True
