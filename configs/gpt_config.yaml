model_params:
    name: 'oc20-2M-GPT2'
    n_positions: 1024
    n_embd: 512
    n_layer: 12
    n_head: 8
    n_epochs: 300
  
data_params:
    tokenizer_path: './data/tokenizer/'
    train_data_path: './data/dataset/oc20-2M/train.txt'
    val_data_path: './data/dataset/oc20-2M/val.txt'
    max_len: 1024
    batch_size: 8
    gradient_accumulation_steps: 16
    
  
  
